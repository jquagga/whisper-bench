{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "549010ea-7cbc-412e-a3f9-3e5351d9402d",
   "metadata": {},
   "source": [
    "# Whisper benchmarking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3ed8721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in ./venv/lib64/python3.12/site-packages (23.3.2)\n",
      "Collecting pip\n",
      "  Using cached pip-24.2-py3-none-any.whl.metadata (3.6 kB)\n",
      "Using cached pip-24.2-py3-none-any.whl (1.8 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 23.3.2\n",
      "    Uninstalling pip-23.3.2:\n",
      "      Successfully uninstalled pip-23.3.2\n",
      "Successfully installed pip-24.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
      "Collecting torch\n",
      "  Using cached https://download.pytorch.org/whl/cpu/torch-2.4.1%2Bcpu-cp312-cp312-linux_x86_64.whl (194.8 MB)\n",
      "Collecting torchvision\n",
      "  Using cached https://download.pytorch.org/whl/cpu/torchvision-0.19.1%2Bcpu-cp312-cp312-linux_x86_64.whl (1.6 MB)\n",
      "Collecting torchaudio\n",
      "  Using cached https://download.pytorch.org/whl/cpu/torchaudio-2.4.1%2Bcpu-cp312-cp312-linux_x86_64.whl (1.7 MB)\n",
      "Collecting filelock (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Collecting typing-extensions>=4.8.0 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
      "Collecting sympy (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
      "Collecting setuptools (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/setuptools-70.0.0-py3-none-any.whl (863 kB)\n",
      "Collecting numpy (from torchvision)\n",
      "  Using cached https://download.pytorch.org/whl/numpy-1.26.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
      "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision)\n",
      "  Using cached https://download.pytorch.org/whl/pillow-10.2.0-cp312-cp312-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Using cached https://download.pytorch.org/whl/MarkupSafe-2.1.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
      "Collecting mpmath>=0.19 (from sympy->torch)\n",
      "  Using cached https://download.pytorch.org/whl/mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, typing-extensions, sympy, setuptools, pillow, numpy, networkx, MarkupSafe, fsspec, filelock, jinja2, torch, torchvision, torchaudio\n",
      "Successfully installed MarkupSafe-2.1.5 filelock-3.13.1 fsspec-2024.2.0 jinja2-3.1.3 mpmath-1.3.0 networkx-3.2.1 numpy-1.26.3 pillow-10.2.0 setuptools-70.0.0 sympy-1.12 torch-2.4.1+cpu torchaudio-2.4.1+cpu torchvision-0.19.1+cpu typing-extensions-4.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip\n",
    "# %pip install --upgrade evaluate jiwer\n",
    "# Let's install torch here.  Pick out your options from https://pytorch.org/get-started/locally/\n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu #CPU Only Version\n",
    "\n",
    "import time\n",
    "# import evaluate\n",
    "\n",
    "# wer = evaluate.load(\"wer\")\n",
    "# cer = evaluate.load(\"cer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834da2b3-051d-4441-a4f6-eef7f062fc73",
   "metadata": {},
   "source": [
    "## Transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6459969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.45.2-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting accelerate\n",
      "  Using cached accelerate-1.0.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in ./venv/lib64/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Using cached huggingface_hub-0.25.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in ./venv/lib64/python3.12/site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib64/python3.12/site-packages (from transformers) (24.1)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Using cached PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2024.9.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting requests (from transformers)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Using cached safetensors-0.4.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers)\n",
      "  Using cached tokenizers-0.20.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Using cached tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: psutil in ./venv/lib64/python3.12/site-packages (from accelerate) (6.0.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in ./venv/lib64/python3.12/site-packages (from accelerate) (2.4.1+cpu)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./venv/lib64/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venv/lib64/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.9.0)\n",
      "Requirement already satisfied: sympy in ./venv/lib64/python3.12/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in ./venv/lib64/python3.12/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib64/python3.12/site-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
      "Requirement already satisfied: setuptools in ./venv/lib64/python3.12/site-packages (from torch>=1.10.0->accelerate) (70.0.0)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->transformers)\n",
      "  Using cached charset_normalizer-3.4.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (34 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->transformers)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->transformers)\n",
      "  Using cached urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->transformers)\n",
      "  Using cached certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib64/python3.12/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./venv/lib64/python3.12/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Using cached transformers-4.45.2-py3-none-any.whl (9.9 MB)\n",
      "Using cached accelerate-1.0.1-py3-none-any.whl (330 kB)\n",
      "Using cached huggingface_hub-0.25.2-py3-none-any.whl (436 kB)\n",
      "Using cached PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (767 kB)\n",
      "Using cached regex-2024.9.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (797 kB)\n",
      "Using cached safetensors-0.4.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (434 kB)\n",
      "Using cached tokenizers-0.20.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "Using cached tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "Using cached charset_normalizer-3.4.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (143 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Installing collected packages: urllib3, tqdm, safetensors, regex, pyyaml, idna, charset-normalizer, certifi, requests, huggingface-hub, tokenizers, accelerate, transformers\n",
      "Successfully installed accelerate-1.0.1 certifi-2024.8.30 charset-normalizer-3.4.0 huggingface-hub-0.25.2 idna-3.10 pyyaml-6.0.2 regex-2024.9.11 requests-2.32.3 safetensors-0.4.5 tokenizers-0.20.1 tqdm-4.66.5 transformers-4.45.2 urllib3-2.2.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35ebe841",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jus10/Projects/whisper-bench/venv/lib64/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are using torch.float32 on cpu with openai/whisper-tiny.en\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jus10/Projects/whisper-bench/venv/lib64/python3.12/site-packages/transformers/models/whisper/generation_whisper.py:496: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time:0.13475316762924194\n",
      " Henry F. Phillips from Wikipedia, the free encyclopedia at en.wicipedia.org. Henry F. Phillips from Wikipedia, the free encyclopedia. Henry F. Phillips, 1890-1958, a U.S. businessman from Portland, Oregon has the honor of having the Phillips-head screw and screwdriver named after him. The importance of the cross-head screw design lies in its self-centering property, useful on automated production lines that use powered screwdrivers. Phillips Major Contribution was in driving the cross-head concept forward to the point where it was adopted by screw makers and automobile companies. Although he received patents for the design in 1936, US patent number 2 million 46,343 US patents 2 million 46,837 to 2 million 46,840. It was so widely copied that by 1949, Phillips lost his patent. The American screw company was responsible for devising a means of manufacturing the screw, and successfully patented and licensed their method. Other screw makers of the 1930s dismissed the Phillips concept since it calls for a relatively complex, recessed socket shape in the head of the screw, as distinct from the simple milled slot of a slotted type screw. The Phillips screw company and the American screw company went on to devise the pause-a-drive screw, which differs from the Phillips in that it is designed to accommodate greater torque than the Philips. An image accompanied this article, caption, Phillips screw head. The following is an info box which accompanies this article. In the box part of the series on screw drive types. Slotted, commonly Early, erroneously flathead. Phillips crosshead. Pause a drive, super drive. Torx. Hex Allen. Robertson. Tri-wing. Torxet. Spannerhead. triple square x z n others polydrive spline drive double hex many images accompanied this info box this page was last modified on 9 April 2008 at 1704 All text is available under the terms of the GNU-Free documentation license. See copy rights for details. Wikipedia is a registered trademark of the Wikimedia Foundation Incorporated, a US-registered 501-C-3 tax-deductible non-profit charity. This sound file and all text in the article are licensed under the new free documentation license. Available at www.GNU.org slash copy left slash FDL.html\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "\n",
    "start_time = time.time()\n",
    "# device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "# torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "device = \"cpu\"\n",
    "torch_dtype = torch.float32\n",
    "\n",
    "model_id = \"openai/whisper-tiny.en\"\n",
    "\n",
    "print(f\"We are using {torch_dtype} on {device} with {model_id}\")\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "sample = \"hp0.wav\"\n",
    "\n",
    "\n",
    "result = pipe(sample, generate_kwargs={\"return_timestamps\": True})\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Elapsed time:{elapsed_time / 60}\")\n",
    "\n",
    "print(result[\"text\"])\n",
    "\n",
    "\n",
    "# with open(\"transformers.txt\", \"w\") as f:\n",
    "#    f.write(str(result[\"text\"].encode(\"utf-8\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afc895c",
   "metadata": {},
   "source": [
    "## Faster Whisper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fc26c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faster-whisper\n",
      "  Using cached faster_whisper-1.0.3-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting av<13,>=11.0 (from faster-whisper)\n",
      "  Using cached av-12.3.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n",
      "Collecting ctranslate2<5,>=4.0 (from faster-whisper)\n",
      "  Using cached ctranslate2-4.4.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.13 in ./venv/lib64/python3.12/site-packages (from faster-whisper) (0.25.2)\n",
      "Requirement already satisfied: tokenizers<1,>=0.13 in ./venv/lib64/python3.12/site-packages (from faster-whisper) (0.20.1)\n",
      "Collecting onnxruntime<2,>=1.14 (from faster-whisper)\n",
      "  Using cached onnxruntime-1.19.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: setuptools in ./venv/lib64/python3.12/site-packages (from ctranslate2<5,>=4.0->faster-whisper) (70.0.0)\n",
      "Requirement already satisfied: numpy in ./venv/lib64/python3.12/site-packages (from ctranslate2<5,>=4.0->faster-whisper) (1.26.3)\n",
      "Requirement already satisfied: pyyaml<7,>=5.3 in ./venv/lib64/python3.12/site-packages (from ctranslate2<5,>=4.0->faster-whisper) (6.0.2)\n",
      "Requirement already satisfied: filelock in ./venv/lib64/python3.12/site-packages (from huggingface-hub>=0.13->faster-whisper) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./venv/lib64/python3.12/site-packages (from huggingface-hub>=0.13->faster-whisper) (2024.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in ./venv/lib64/python3.12/site-packages (from huggingface-hub>=0.13->faster-whisper) (24.1)\n",
      "Requirement already satisfied: requests in ./venv/lib64/python3.12/site-packages (from huggingface-hub>=0.13->faster-whisper) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./venv/lib64/python3.12/site-packages (from huggingface-hub>=0.13->faster-whisper) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venv/lib64/python3.12/site-packages (from huggingface-hub>=0.13->faster-whisper) (4.9.0)\n",
      "Collecting coloredlogs (from onnxruntime<2,>=1.14->faster-whisper)\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime<2,>=1.14->faster-whisper)\n",
      "  Using cached flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting protobuf (from onnxruntime<2,>=1.14->faster-whisper)\n",
      "  Using cached protobuf-5.28.2-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: sympy in ./venv/lib64/python3.12/site-packages (from onnxruntime<2,>=1.14->faster-whisper) (1.12)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime<2,>=1.14->faster-whisper)\n",
      "  Using cached humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib64/python3.12/site-packages (from requests->huggingface-hub>=0.13->faster-whisper) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib64/python3.12/site-packages (from requests->huggingface-hub>=0.13->faster-whisper) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib64/python3.12/site-packages (from requests->huggingface-hub>=0.13->faster-whisper) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib64/python3.12/site-packages (from requests->huggingface-hub>=0.13->faster-whisper) (2024.8.30)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./venv/lib64/python3.12/site-packages (from sympy->onnxruntime<2,>=1.14->faster-whisper) (1.3.0)\n",
      "Using cached faster_whisper-1.0.3-py3-none-any.whl (2.0 MB)\n",
      "Using cached av-12.3.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.7 MB)\n",
      "Using cached ctranslate2-4.4.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.6 MB)\n",
      "Using cached onnxruntime-1.19.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.2 MB)\n",
      "Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Using cached flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Using cached protobuf-5.28.2-cp38-abi3-manylinux2014_x86_64.whl (316 kB)\n",
      "Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Installing collected packages: flatbuffers, protobuf, humanfriendly, ctranslate2, av, coloredlogs, onnxruntime, faster-whisper\n",
      "Successfully installed av-12.3.0 coloredlogs-15.0.1 ctranslate2-4.4.0 faster-whisper-1.0.3 flatbuffers-24.3.25 humanfriendly-10.0 onnxruntime-1.19.2 protobuf-5.28.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install faster-whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d2da149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language 'en' with probability 1.000000\n",
      " Henry F. Phillips from Wikipedia, the free encyclopedia at en.wickopedia.org. Henry F. Phillips from Wikipedia, the free encyclopedia. Henry F. Phillips, 1890-1958, a U.S. businessman from Portland, Oregon has the honor of having the Phillips head screw and screwdriver named after him. The importance of the cross-head screw design lies in its self-centering property, useful on automated production lines that use powered screwdrivers. Phillips's major contribution was in driving the cross-head concept forward to the point where it was adopted by screw makers and automobile companies. Although he received patents for the design in 1936, U.S. patent number two million forty six thousand three hundred forty three U.S. patents two million forty six thousand eight hundred thirty seven to two million forty six thousand eight hundred forty, it was so widely copied that by 1949 Phillips lost his patent. The American screw company was responsible for devising a means of manufacturing the screw and successfully patented and licensed their method. Other screw makers of the 1930s dismissed the Phillips concept since it calls for a relatively complex recessed socket shape in the head of the screw, as distinct from the simple milled slot of a slotted type screw. The Phillips screw company and the American screw company went on to devise the pause-a-drive screw, which differs from the Phillips in that it is designed to accommodate greater torque than the Phillips. An image accompanied this article, caption Phillips screw head. The following is an info box which accompanies this article. Info box part of the series on screw-drive types, slotted commonly erroneously flathead, Phillips crosshead, pause-a-drive, super-drive, torques, hex, Allen, Robertson, tri-wing, torqueset, spannerhead, triple square, x, z, n. Others, poly-drive, spline-drive, double hex. Many images accompanied this info box. This page was last modified on 9 April 2008 at 1704. All text is available under the terms of the GNU-free documentation license. See copyrights for details. Wikipedia is a registered trademark of the Wikimedia Foundation incorporated, a US-registered 501C3 tax-deductible non-profit charity. This sound file and all text in the article are licensed under the GNU-free documentation license. Available at www.gnu.org slash copy left slash f-d-l dot h-t-m-l.\n",
      "Elapsed time:0.08412132263183594\n"
     ]
    }
   ],
   "source": [
    "from faster_whisper import WhisperModel\n",
    "\n",
    "start_time = time.time()\n",
    "model_size = \"tiny.en\"\n",
    "# model_size = \"deepdml/faster-whisper-large-v3-turbo-ct2\"\n",
    "\n",
    "# Run on GPU with FP16\n",
    "# model = WhisperModel(model_size, device=\"cuda\", compute_type=\"float16\")\n",
    "\n",
    "# or run on GPU with INT8\n",
    "# model = WhisperModel(model_size, device=\"cuda\", compute_type=\"int8_float16\")\n",
    "# or run on CPU with INT8\n",
    "model = WhisperModel(model_size, device=\"cpu\", compute_type=\"auto\")\n",
    "\n",
    "segments, info = model.transcribe(sample, beam_size=5)\n",
    "\n",
    "print(\n",
    "    \"Detected language '%s' with probability %f\"\n",
    "    % (info.language, info.language_probability)\n",
    ")\n",
    "\n",
    "calltext = \"\".join(segment.text for segment in segments)\n",
    "\n",
    "print(calltext)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Elapsed time:{elapsed_time / 60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc3e2ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f0538e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
